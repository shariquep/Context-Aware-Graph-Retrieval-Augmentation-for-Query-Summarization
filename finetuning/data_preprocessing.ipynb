{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b6eae49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\perva\\anaconda3\\envs\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "Dataset loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "token = \"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\"\n",
    "\n",
    "# Login to Huggingface\n",
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()\n",
    "\n",
    "# Load the dataset\n",
    "huggingface_dataset_name = \"hotpot_qa\"\n",
    "dataset = load_dataset(huggingface_dataset_name, \"distractor\", trust_remote_code=True)\n",
    "print(\"Dataset loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77246b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\perva\\anaconda3\\envs\\cuda\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\perva\\anaconda3\\envs\\cuda\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\perva\\.cache\\huggingface\\hub\\models--microsoft--phi-2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files: 100%|██████████| 2/2 [05:15<00:00, 157.60s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure model and tokenizer\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "device_map = {\"cuda\": 0}\n",
    "\n",
    "model_name = 'microsoft/phi-2'\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)\n",
    "\n",
    "print(\"Model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c0739ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found max length: 2048\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side=\"left\", add_eos_token=True, add_bos_token=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(model_name, add_bos_token=True, trust_remote_code=True, use_fast=False)\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "\n",
    "def get_max_length(model):\n",
    "    conf = model.config\n",
    "    max_length = None\n",
    "    for length_setting in [\"n_positions\", \"max_position_embeddings\", \"seq_length\"]:\n",
    "        max_length = getattr(model.config, length_setting, None)\n",
    "        if max_length:\n",
    "            print(f\"Found max length: {max_length}\")\n",
    "            break\n",
    "    if not max_length:\n",
    "        max_length = 1024\n",
    "        print(f\"Using default max length: {max_length}\")\n",
    "    return max_length\n",
    "\n",
    "MAX_LENGTH = get_max_length(original_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13740e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(batch, tokenizer, max_length):\n",
    "    \"\"\"Tokenizing a batch\"\"\"\n",
    "    return tokenizer(\n",
    "        batch[\"truncated_combined_text\"],\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "def build_context(data_point):\n",
    "    supporting_titles = data_point['supporting_facts']['title']\n",
    "    supporting_sent_ids = data_point['supporting_facts']['sent_id']\n",
    "\n",
    "    relevant_context = []\n",
    "    other_context = []\n",
    "\n",
    "    for i, title in enumerate(data_point['context']['title']):\n",
    "        if title in supporting_titles:\n",
    "            relevant_sentences = [\n",
    "                sentence for j, sentence in enumerate(data_point['context']['sentences'][i])\n",
    "                if title in supporting_titles and j in supporting_sent_ids\n",
    "            ]\n",
    "            relevant_context.extend(relevant_sentences)\n",
    "        else:\n",
    "            other_context.extend(data_point['context']['sentences'][i])\n",
    "\n",
    "    # Randomly sample the remaining context to pad the relevant context\n",
    "    random.shuffle(other_context)\n",
    "    return relevant_context, other_context\n",
    "\n",
    "def truncate_context(data_point, max_length=2048):\n",
    "    relevant_context, other_context = build_context(data_point)\n",
    "\n",
    "    instruction_prompt = \"### Instruct: With the given context, please answer the question in one word.\"\n",
    "    question = data_point['question']\n",
    "    answer = data_point['answer']\n",
    "    context_key = \"Context:\"\n",
    "    response_key = \"### Output:\"\n",
    "\n",
    "    # Calculate the length of static parts\n",
    "    static_parts = f\"{instruction_prompt}\\nQuestion: {question}\\n{context_key}\\n{response_key}\\n{answer}\"\n",
    "    static_parts_length = len(tokenizer.tokenize(static_parts))\n",
    "\n",
    "    # Calculate the available length for the context\n",
    "    available_length = max_length - static_parts_length\n",
    "\n",
    "    # Ensure available_length is non-negative\n",
    "    if available_length <= 0:\n",
    "        raise ValueError(\"Static parts of the prompt exceed the max length.\")\n",
    "\n",
    "    # Tokenize relevant context and check length\n",
    "    relevant_context_str = ' '.join(relevant_context)\n",
    "    relevant_tokens = tokenizer(relevant_context_str, return_tensors='pt').input_ids[0]\n",
    "\n",
    "    if len(relevant_tokens) > available_length:\n",
    "        # Truncate relevant context if it exceeds available length\n",
    "        truncated_relevant_tokens = relevant_tokens[:available_length]\n",
    "        truncated_relevant_context = tokenizer.decode(truncated_relevant_tokens, skip_special_tokens=True)\n",
    "        truncated_combined_text = f\"{instruction_prompt}\\nQuestion: {question}\\n{context_key}\\n{truncated_relevant_context}\\n{response_key}\\n{answer}\"\n",
    "    else:\n",
    "        # If relevant context fits, add as much irrelevant context as possible\n",
    "        truncated_relevant_context = relevant_context_str\n",
    "        remaining_length = available_length - len(relevant_tokens)\n",
    "        irrelevant_context_str = ' '.join(other_context)\n",
    "        irrelevant_tokens = tokenizer(irrelevant_context_str, return_tensors='pt', truncation=True, max_length=remaining_length).input_ids[0]\n",
    "        truncated_irrelevant_context = tokenizer.decode(irrelevant_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Combine relevant and truncated irrelevant context\n",
    "        combined_context = truncated_relevant_context + ' ' + truncated_irrelevant_context\n",
    "        combined_context_list = combined_context.split('. ')\n",
    "        random.shuffle(combined_context_list)\n",
    "        shuffled_combined_context = '.'.join(combined_context_list)\n",
    "\n",
    "        truncated_combined_text = f\"{instruction_prompt}\\nQuestion: {question}\\n{context_key}\\n{shuffled_combined_context}\\n{response_key}\\n{answer}\"\n",
    "\n",
    "    final_tokens = tokenizer(truncated_combined_text, return_tensors='pt').input_ids[0]\n",
    "    if len(final_tokens) > max_length:\n",
    "        truncated_combined_text = tokenizer.decode(final_tokens[:max_length-2], skip_special_tokens=True)\n",
    "\n",
    "    # Update the data_point with the truncated context\n",
    "    data_point['truncated_combined_text'] = truncated_combined_text\n",
    "    return data_point\n",
    "\n",
    "def process_dataset(dataset, max_length=MAX_LENGTH):\n",
    "    processed_dataset = dataset.map(lambda x: truncate_context(x, max_length), batched=False)\n",
    "    return processed_dataset\n",
    "\n",
    "def check_lengths(dataset, max_length=MAX_LENGTH):\n",
    "    for data_point in dataset:\n",
    "        tokens = tokenizer(data_point['truncated_combined_text'], return_tensors='pt')\n",
    "        if tokens.input_ids.shape[1] > max_length:\n",
    "            print(f\"Sequence length {tokens.input_ids.shape[1]} exceeds max length {max_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664d1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the train and validation datasets\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "\n",
    "# Sample 20,000 rows from the train split and process\n",
    "sampled_train_dataset = train_dataset.shuffle(seed=42).select(range(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99359e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 37/20000 [00:02<20:32, 16.20 examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2052 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Map: 100%|██████████| 20000/20000 [24:45<00:00, 13.46 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7405/7405 [05:52<00:00, 20.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing train dataset...\")\n",
    "sampled_train_dataset = process_dataset(sampled_train_dataset)\n",
    "print(\"Processing validation dataset...\")\n",
    "validation_dataset = process_dataset(validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de7de0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking lengths of train dataset...\n",
      "Checking lengths of validation dataset...\n"
     ]
    }
   ],
   "source": [
    "# Check the lengths after processing\n",
    "print(\"Checking lengths of train dataset...\")\n",
    "check_lengths(sampled_train_dataset)\n",
    "print(\"Checking lengths of validation dataset...\")\n",
    "check_lengths(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64b14d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 110292.25 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 7405/7405 [00:00<00:00, 141487.09 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing and saving complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the processed datasets\n",
    "sampled_train_dataset.save_to_disk(\"processed_train_dataset_phi2\")\n",
    "validation_dataset.save_to_disk(\"processed_validation_dataset_phi2\")\n",
    "\n",
    "print(\"Preprocessing and saving complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d1d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruct: With the given context, please answer the question in one word.\n",
      "Question: Which airport is located in Maine, Sacramento International Airport or Knox County Regional Airport?\n",
      "Context:\n",
      "   NCT is the 3rd busiest TRACON in America.   It is home to a thriving lobster fishery and hosts a summer colony.   The cause of the accident was controlled flight into terrain (CFIT) after the failure of the flightcrew to stop the aircraft's descent below the minimum descent altitude for the nonprecision approach at Knox County airport.   The population was 355 at the 2010 census.   The plantation is both a year-round island community and a summer colony.   San Francisco International Airport is the 2nd largest airport in California and the largest airport serving Northern California.   Located in Rancho Cordova near Sacramento, NCT controls airspace over 19000 square miles, and serves Reno International Airport, Sacramento International Airport, San Jose International Airport, Oakland International Airport, and San Francisco International Airport, plus 19 other smaller airports with air traffic control towers.   North Haven is accessed by three-times daily ferry service from Rockland, or by air taxi from Knox County Regional Airport.   The population was 74 at the 2010 census.Knox County Regional Airport (IATA: RKD, ICAO: KRKD, FAA LID: RKD) is a county owned, public use airport in Knox County, Maine, United States.   The population was 1,580 at the 2010 census.Sacramento International Airport (IATA: SMF, ICAO: KSMF, FAA LID: SMF) is 10 mi northwest of downtown Sacramento, in Sacramento County, California.   The town is both a year-round island community and a prominent summer colony.   Since there is no bridge to the island, Vinalhaven is accessible from Rockland via an approximately hour-and-fifteen-minute ferry ride across West Penobscot Bay, or by air taxi from Knox County Regional Airport.Raleigh Exec: The Raleigh Executive Jetport @ Sanford-Lee County or Raleigh Exec Jetport at Sanford-Lee CountyFAA Airport Master Record for TTA (Form 5010 ) (ICAO: KTTA, FAA LID: TTA) is a public use airport located seven nautical miles (8 mi, 13 km) northeast of the central business district of Sanford, a city in Lee County, North Carolina, United States.Downeast Airlines Flight 46 was a scheduled airline service in the United States from Boston's Logan International Airport to Rockland, Maine operated by Downeast Airlines. It is owned by the Sanford-Lee County Regional Airport Authority and was previously known as Sanford-Lee County Regional Airport.   It includes the village of Ash Point.   The island is located within Penobscot Bay about 20 miles east of the mainland coast and is accessible by ferry from Rockland or by air taxi from Knox County Regional Airport.   Vinalhaven is also used to refer to the Island itself.   NorCal TRACON is the step between local control (in an airport's control tower) and Air Route Traffic Control Center (ARTCC), in this case, Oakland Center (ICAO code: ZOA).   On May 30, 1979 a de Havilland Canada DHC-6 Twin Otter operating the flight crashed during a nonprecision approach to Rockland's Knox County Regional Airport.   This airport is included in the National Plan of Integrated Airport Systems for 2011–2015, which categorized it as a \"reliever airport\" for Raleigh-Durham International Airport.   The population was 1,165 at the 2010 census.   It is an FAA certified commercial airport served by United Airlines' affiliate with daily regional flights.Owls Head is a town in Knox County, Maine, United States.Matinicus Isle is an island plantation in Knox County, Maine, United States.   A resort and fishing area, the community is home to the Knox County Regional Airport.Lea County Regional Airport (IATA: HOB, ICAO: KHOB) (Lea County-Hobbs Airport) is four miles (6.4 km) west of Hobbs, in Lea County, New Mexico.   Lea County Regional Airport is the largest of the three airports owned and operated by Lea County Government.Vinalhaven is a town located on the larger of the two Fox Islands in Knox County, Maine, United States.   The airport covers 898 acre and has three runways.   Lea County also owns and operated two general aviation airports in Lovington and Jal, New Mexico..Northern California TRACON (NCT) (Terminal Radar Approach Control), or NorCal TRACON for short, is an air traffic control facility that provides safety alerts, separation, and sequencing of air traffic arriving, departing, and transiting the airspace and airports in Northern California.   The investigation into the accident looked into the airline's corporate culture as a contributing factor to the crash; this was the first time an investigation took this approach to an air crash.North Haven is a town in Knox County, Maine, United States, in Penobscot Bay\n",
      "### Output:\n",
      "Knox County Regional Airport\n"
     ]
    }
   ],
   "source": [
    "for data_point in sampled_train_dataset:\n",
    "    print(data_point['truncated_combined_text'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b99c582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
